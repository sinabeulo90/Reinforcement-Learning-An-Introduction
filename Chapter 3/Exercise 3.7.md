## Exercise 3.7

### Question:

미로를 달리는 로봇을 설계한다고 가정해 보자. 미로를 탈출하면 로봇에게 +1의 보상을 주고 그 밖의 보상은 0을 주기로 결정했다고 해 보자. 이 작업은 자연스럽게 여러 개의 에피소드, 즉 미로를 달리는 연속적인 시도로 나누어지는 것처럼 보인다. 그래서 이 작업을 보상의 총합에 대한 기댓값(식 3.7)을 최대화하는 것을 목표로 하는 에피소딕 작업으로 다루기로 결정했다고 해 보자. 잠깐 동안 로봇을 학습시킨 후에도 로봇이 미로를 탈출하는 데 있어 개선되는 모습을 보이지 못한다고 해 보자. 무엇이 잘못되었는가? 로봇에게 이루고자 하는 것이 무엇인지 효과적으로 전달했는가?

### Answer:

미래를 탈출한 경로에 대해 보상의 총합에 대한 기댓값(식 3.7)은 항상 +1을 가진다. 잠깐 동안 로봇을 학습시킨 후 새로운 에피소드를 진행할 때, 탈출이 성공한 경로에 대한 이득만이 1이 되고 그 이외의 상태는 모두 0이 된다. 로봇이 미로를 탈출하기 위해 이미 탈출이 성공한 경로만 선택하게 되므로, 특별한 개선의 모습을 보이지 않는다. 더 빠르게 미로를 탈출하기 위해서는, 각 시간 단계마다 보상에 할인율을 적용하여 빠르게 탈출할수록 보상의 총합을 높이는 방법을 사용한다.